{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "torchvision.set_image_backend('accimage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "data_root = Path(\"/home/hugo/ai/datasets/celeba-aligned-cropped\")\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 8\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "# size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "num_channels = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "z_latent_size = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "gen_feat_size = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "disc_feat_size = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(dsets.ImageFolder):\n",
    "    # Override DatasetFolder's find_classes method since\n",
    "    # we don't want any classes here\n",
    "    def find_classes(self, directory: str):\n",
    "        return None, {\"\":\"\"}\n",
    "\n",
    "dataset = Dataset(\n",
    "    root=data_root.as_posix(),\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the device we want to run on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if cuda_available:\n",
    "    print(\"Cuda is available! ü•≥\")\n",
    "else:\n",
    "    print(\"‚ùå Cuda is unavailable\")\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda:0\" if (cuda_available and use_gpu) else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(\n",
    "    np.transpose(\n",
    "        vutils.make_grid(\n",
    "            real_batch[0].to(device)[:64],\n",
    "            padding=2,\n",
    "            normalize=True,\n",
    "        ).cpu(),\n",
    "        (1, 2, 0),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights initialization\n",
    "\n",
    "In the DCGAN paper, the authors specify that the weights should all be initialized with mean = 0 and std = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        m.weight.data.normal_(.0, .02)\n",
    "        # Same as nn.init.normal_(m.weight.data, .0, .02)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.normal_(1., .02)\n",
    "        m.weight.data.zero_() # Same as .fill_(0)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    print(f\"üîß {m.__class__.__name__} weights initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # Input is z (latent vector), going into a convolution\n",
    "            nn.ConvTranspose2d(z_latent_size, gen_feat_size * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(gen_feat_size * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # State size: (gen_feat_size * 8) x 4 x 4\n",
    "            nn.ConvTranspose2d(gen_feat_size * 8, gen_feat_size * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(gen_feat_size * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # State size: (gen_feat_size * 4) x 8 x 8\n",
    "            nn.ConvTranspose2d(gen_feat_size * 4, gen_feat_size * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(gen_feat_size * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # State size: (gen_feat_size * 2) x 16 x 16\n",
    "            nn.ConvTranspose2d(gen_feat_size * 2, gen_feat_size, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(gen_feat_size * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # State size: gen_feat_size x 32 x 32\n",
    "            nn.ConvTranspose2d(gen_feat_size, num_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "            \n",
    "            # State size: num_channels x 64 x 64\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator\n",
    "gen_net = Generator().to(device)\n",
    "\n",
    "# Initialize all weights to random values (mean=0, std=0.02)\n",
    "gen_net.apply(init_weights)\n",
    "\n",
    "gen_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # Input is num_channels x 64 x 64\n",
    "            nn.Conv2d(num_channels, disc_feat_size, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=.2, inplace=True),\n",
    "            \n",
    "            # State size: disc_feat_size x 32 x 32\n",
    "            nn.Conv2d(disc_feat_size, disc_feat_size * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(disc_feat_size * 2),\n",
    "            nn.LeakyReLU(negative_slope=.2, inplace=True),\n",
    "            \n",
    "            # State size: (disc_feat_size * 2) x 16 x 16\n",
    "            nn.Conv2d(disc_feat_size * 2, disc_feat_size * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(disc_feat_size * 4),\n",
    "            nn.LeakyReLU(negative_slope=.2, inplace=True),\n",
    "            \n",
    "            # State size: (disc_feat_size * 4) x 8 x 8\n",
    "            nn.Conv2d(disc_feat_size * 4, disc_feat_size * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(disc_feat_size * 8),\n",
    "            nn.LeakyReLU(negative_slope=.2, inplace=True),\n",
    "            \n",
    "            # State size: (disc_feat_size * 8) x 4 x 4\n",
    "            nn.Conv2d(disc_feat_size * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator\n",
    "disc_net = Discriminator().to(device)\n",
    "\n",
    "# Initialize all weights to random values (mean=0, std=0.02)\n",
    "disc_net.apply(init_weights)\n",
    "\n",
    "disc_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss, fixed noise & optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCE (Binary CrossEntropy) loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Some fixed settings for training\n",
    "fixed_noise = torch.randn(64, z_latent_size, 1, 1, device=device) # Static eval noise\n",
    "real = 1.\n",
    "fake = 0.\n",
    "\n",
    "# Adam optimizers\n",
    "gen_optimizer = optim.Adam(gen_net.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "disc_optimizer = optim.Adam(disc_net.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list, gen_losses, disc_losses = [], [], []\n",
    "num_iterations = 0\n",
    "\n",
    "print(\"üîÑ Starting training loop\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, (data, _) in enumerate(dataloader): # _ is the label vector (batched) we are ignoring\n",
    "        \n",
    "        # 1. Update discriminator network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        gen_net.zero_grad()\n",
    "        \n",
    "        # Format batch\n",
    "        img_batch = data.to(device)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ff75fa74c180f1da93b90ae96812717cb33e5db90a115128964aff1e3c3dd26"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
